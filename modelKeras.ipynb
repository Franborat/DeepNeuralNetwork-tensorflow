{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "%run 1.ReadingData.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTS = x_train.shape[1]\n",
    "OUTPUTS = t_train.shape[1]\n",
    "NUM_TRAINING_EXAMPLES = int(round(x_train.shape[0]/1))\n",
    "NUM_DEV_EXAMPLES = int (round (x_dev.shape[0]/1))\n",
    "NUM_TEST_EXAMPLES = int (round (x_test.shape[0]/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping\n",
    "from keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor='val_acc', patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "examples = [1, 2, 3]\n",
    "for example in examples:\n",
    "    for l in range(example):\n",
    "        print(\"Hola\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from time import time\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch with TensorBoard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [3,4]\n",
    "layer_sizes = [64]\n",
    "learning_rates = [0.001]\n",
    "#learning_rates = [1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1]\n",
    "batch_sizes = [32]\n",
    "#dropout_rates = [0.1,0.2,0.3,0.4,0.5]\n",
    "dropout_rates = [0.6]\n",
    "\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for learning_rate in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                for dropout_rate in dropout_rates:\n",
    "        \n",
    "                    NAME = \"{}-dense-{}-nodes-{}-learning_rate-{}-batch_size-{}-dropout_rate-{}\".format(dense_layer, layer_size, learning_rate, batch_size, dropout_rate, int(time()))\n",
    "                    print(NAME)\n",
    "                    \n",
    "                    mlp = Sequential()\n",
    "                    \n",
    "                    mlp.add(Dense(layer_size, input_dim=INPUTS))\n",
    "                    mlp.add(Activation('relu'))\n",
    "                    mlp.add(Dropout(0.2))\n",
    "                    \n",
    "                    for l in range(dense_layer-1):\n",
    "                        mlp.add(Dense(layer_size))\n",
    "                        mlp.add(Activation('relu'))\n",
    "                        mlp.add(Dropout(0.2))\n",
    "                        \n",
    "                    # Final layer\n",
    "                    mlp.add(Dense(OUTPUTS))\n",
    "                    mlp.add(Dropout(0.2))\n",
    "                    mlp.add(Activation('softmax'))\n",
    "                    \n",
    "                    tensorboard = TensorBoard(log_dir=\"BatchSize/{}\".format(NAME))\n",
    "                    \n",
    "                    \n",
    "                    opt = optimizers.Adam(lr=learning_rate)\n",
    "                    mlp.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                    mlp.summary()    \n",
    "                    mlp.fit(x_train, t_train, batch_size=batch_size, epochs=300, verbose=2, validation_data=(x_dev, t_dev), callbacks=[tensorboard, earlystop])\n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Results:\n",
    "\n",
    "    With 3x64:\n",
    "    \n",
    "    * 4x64 --> 0.3 dropout\n",
    "\n",
    "    With 4x64:\n",
    "    \n",
    "    * Surprisingly --> 0.1 and 0.5(best) dropout\n",
    "    \n",
    "Overall: 3x64 --> 0,3 dropout (3 dense layers outperform 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "mlp = Sequential()\n",
    "\n",
    "# Middle layers\n",
    "\n",
    "# First\n",
    "mlp.add(Dense(n_neurons_per_layer[0], input_dim=INPUTS))\n",
    "mlp.add(Activation('relu'))\n",
    "mlp.add(Dropout(0.2))\n",
    "\n",
    "# Second\n",
    "mlp.add(Dense(n_neurons_per_layer[1]))\n",
    "mlp.add(Activation('relu'))\n",
    "mlp.add(Dropout(0.2))\n",
    "\n",
    "# Second\n",
    "mlp.add(Dense(n_neurons_per_layer[2]))\n",
    "mlp.add(Activation('relu'))\n",
    "mlp.add(Dropout(0.2))\n",
    "\n",
    "# Final layer\n",
    "mlp.add(Dense(OUTPUTS))\n",
    "mlp.add(Activation('softmax'))\n",
    "\n",
    "opt = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "mlp.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mlp.fit(x_train, t_train, batch_size=batch_size, epochs=n_epochs, verbose=2, validation_data=(x_dev, t_dev), callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "loss, acc = mlp.evaluate(x_test, t_test, verbose=0)\n",
    "end = time()\n",
    "print('MLP took ' + str(end - start) + ' seconds')\n",
    "print('Test loss: ' + str(loss) + ' - Accuracy: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(x_train, t_train, batch_size=batch_size, epochs=n_epochs, verbose=2, validation_data=(x_dev, t_dev), callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
